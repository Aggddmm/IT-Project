{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import sounddevice as sd\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "device = \"mps\"\n",
    "model_path = \"Your model path\"\n",
    "sampling_rate = 16000\n",
    "duration = 1\n",
    "audio_queue = queue.Queue()\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "RECORD_THRESHOLD = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definitions\n",
    " - audio_callback(indata)                     (Enqueue Audio Data)\n",
    " - record_audio(duration, sampling_rate)      (Automatic Audio Recording)\n",
    " - process_audio_stream(sampling_rate)        (Speech Recognition)\n",
    " - debug_player()                             (Play what just recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_callback(indata):\n",
    "    audio_queue.put(indata.copy())  # Put the captured audio \n",
    "\n",
    "# record audio trunk into queue\n",
    "def record_audio(duration, sampling_rate):\n",
    "    can_record:bool = False\n",
    "    sound_amp_queue = queue.Queue()\n",
    "    \n",
    "    input_stream = sd.InputStream(channels=1, samplerate=sampling_rate)\n",
    "    input_stream.start()\n",
    "    try:\n",
    "        while True:\n",
    "            # detect sound amplitude to determine if we should record\n",
    "            if (sound_amp_queue.qsize() > 15):\n",
    "                sound_amp_queue.get()\n",
    "            # Data part\n",
    "            data, overflowed = input_stream.read(sampling_rate * duration)\n",
    "            volume_norm = np.linalg.norm(data) * 10\n",
    "            \n",
    "            \"\"\"This code section can dynamically adapt ambient noise and determine \"\"\"\n",
    "            if sound_amp_queue.qsize() < 3:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "                continue\n",
    "            # Determine Part\n",
    "            sound_amp = sum(sound_amp_queue.queue) / sound_amp_queue.qsize()\n",
    "            if abs(volume_norm - sound_amp) > RECORD_THRESHOLD:\n",
    "                can_record = True\n",
    "            else:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "\n",
    "            # print(\"Sound amplitude: \", volume_norm, \"Sound average: \", sound_amp)\n",
    "            # Record part\n",
    "            if can_record:\n",
    "                print(\"Recording...\")\n",
    "            audio_array = np.empty((0, 1)) \n",
    "            record_amp_queue = queue.Queue()\n",
    "            while can_record:\n",
    "                audio_array = np.append(audio_array, data)\n",
    "                data, overflowed = input_stream.read(sampling_rate * duration)\n",
    "                rec_volume_norm = np.linalg.norm(data) * 10\n",
    "                # determine when to stop recording\n",
    "                record_amp_queue.put(rec_volume_norm)\n",
    "                if record_amp_queue.qsize() > 3:\n",
    "                    record_amp = sum(record_amp_queue.queue) / record_amp_queue.qsize()\n",
    "                    # terminate recording if the amplitude back to normal\n",
    "                    if abs(sound_amp - record_amp) < RECORD_THRESHOLD:\n",
    "                        can_record = False\n",
    "                        audio_callback(audio_array)\n",
    "                        print(\"Recording stopped.\")\n",
    "                        break\n",
    "                    record_amp_queue.get()\n",
    "                # audio_array = np.append(audio_array, data)\n",
    "            \n",
    "    finally:\n",
    "        input_stream.stop()  \n",
    "        input_stream.close()  \n",
    "    \n",
    "def process_audio_stream(sampling_rate):\n",
    "    \"\"\"Continuously processes audio chunks from the queue and transcribes them.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            audio_data = audio_queue.get(timeout=1) \n",
    "        except queue.Empty:\n",
    "            time.sleep(1)\n",
    "            continue  \n",
    "        # construct the audio input\n",
    "        audio_input = {\"array\": audio_data, \"sampling_rate\": sampling_rate}\n",
    "        result = pipe(audio_input)\n",
    "        print(\"    ----> Result: \" + result[\"text\"], end=\" \")\n",
    "        audio_queue.task_done()\n",
    "\n",
    "def debug_player():\n",
    "    \"\"\"Debug function to play the audio from the queue.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            audio_data = audio_queue.get()\n",
    "        except queue.Empty:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        print(\"Playing audio...\")\n",
    "        sd.play(audio_data, sampling_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-thread Recording & Speech Recognition\n",
    "i.e. Code Demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create threads for recording and processing\n",
    "recording_thread = threading.Thread(target=record_audio, args=[duration, sampling_rate])\n",
    "processing_thread = threading.Thread(target=process_audio_stream, args=[sampling_rate])\n",
    "# debug_thread = threading.Thread(target=debug_player)\n",
    "# Start the threads\n",
    "recording_thread.start()\n",
    "processing_thread.start()\n",
    "# debug_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPS-Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
