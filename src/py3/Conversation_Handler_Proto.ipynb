{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal imports\n",
    "from LM import LMBackend\n",
    "from TTS import TTSBackend\n",
    "from SpeechRecognition import SpeechRecognitionBackend\n",
    "from QuestionClassifier import QuestionClassifierBackend\n",
    "\n",
    "# external imports\n",
    "import queue\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "audio_queue = queue.Queue()\n",
    "# Constants\n",
    "RECORD_THRESHOLD = 200\n",
    "SAMPLING_RATE = 16000\n",
    "REC_DURATION = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start All Backend Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Language Model Backend\n",
    "lm_instance = LMBackend()\n",
    "lm_instance.init(\"/Users/lipeihong/Desktop/IT Project/py3/Language_Model/LM/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TTS Backend\n",
    "tts_instance = TTSBackend()\n",
    "tts_instance.init(\"espnet/fastspeech2_conformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Speech Recognition Backend\n",
    "sr_instance = SpeechRecognitionBackend()\n",
    "sr_instance.init(\"/Users/lipeihong/Downloads/whisper-small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.3.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GridSearchCV from version 1.3.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'historicalQuestion'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Question Classifier Backend\n",
    "qc_instance = QuestionClassifierBackend()\n",
    "qc_instance.init(\"q_classification_model\")\n",
    "\n",
    "qc_instance.classify(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Functions of Chatting Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\"\"\" These functions are from Speech-Recognition branch, with process_audio_stream modified to utlise all backends to generate voice response \"\"\"\n",
    "\"\"\" Different ways to coordinate these functions. \"\"\"\n",
    "\n",
    "def audio_callback(indata):\n",
    "    audio_queue.put(indata.copy())  # Put the captured audio\n",
    "\n",
    "def get_mic_amplitude(input_stream, duration):\n",
    "    data, overflowed = input_stream.read(SAMPLING_RATE * duration)\n",
    "    return np.linalg.norm(data) * 10\n",
    "\n",
    "def process_audio_stream(audio_input:dict) -> None:\n",
    "    \"\"\" from voice input to voice response \"\"\"\n",
    "    # recognize audio\n",
    "    sr_result = sr_instance.recognize(audio_input)[\"text\"]\n",
    "    print(\"**** Debug ****: \", sr_result)\n",
    "    # classify question\n",
    "    question_type = qc_instance.classify(sr_result)\n",
    "    \n",
    "    if question_type == \"historicalQuestion\":\n",
    "        # TODO: add historical question answering pipeline.\n",
    "        # Logic: 1. Serch clue in database\n",
    "        #        2. Generate prompt\n",
    "        #        3. update sr_result, which is the prompt.\n",
    "        pass\n",
    "    \n",
    "    # generate response\n",
    "    lm_result = lm_instance.generate_text(sr_result)\n",
    "    \n",
    "    # generate voice response\n",
    "    tts_result = tts_instance.synthesize(lm_result)\n",
    "    \n",
    "    # play audio\n",
    "    sd.play(tts_result[\"array\"], samplerate=tts_result[\"sampling_rate\"])\n",
    "    sd.wait()\n",
    "\n",
    "def debug_player(audio_data):\n",
    "    \"\"\"Debug function to play the audio from the queue.\"\"\"\n",
    "    print(\"Playing audio...\")\n",
    "    sd.play(audio_data, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedicated Debug Block For main function\n",
    "cuz bugs really easy to 'be produced' in this block...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" This function controls when to record and when to stop recording \"\"\"\n",
    "    voice_input_stream = sd.InputStream(channels=1, samplerate=SAMPLING_RATE)\n",
    "    voice_input_stream.start()\n",
    "    \n",
    "    sound_amp_queue = queue.Queue()\n",
    "    can_record:bool = False\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # detect sound amplitude to determine if we should record\n",
    "            if (sound_amp_queue.qsize() > 15):\n",
    "                sound_amp_queue.get()\n",
    "            \n",
    "            data, overflowed = voice_input_stream.read(SAMPLING_RATE * REC_DURATION)\n",
    "            volume_norm = np.linalg.norm(data) * 10\n",
    "            \n",
    "            if sound_amp_queue.qsize() < 3:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "                continue\n",
    "            \n",
    "            # print(\"amplitude queue: \", sound_amp_queue.queue)\n",
    "            avg_mic_amplitude = sum(sound_amp_queue.queue) / sound_amp_queue.qsize()\n",
    "            # only collect background noise level, not outliers.\n",
    "            if abs(volume_norm - avg_mic_amplitude) > RECORD_THRESHOLD:\n",
    "                can_record = True\n",
    "            else:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "            \n",
    "            if can_record:\n",
    "                print(\"[+] Recording...\")\n",
    "                audio_array = np.empty((0, 1)) \n",
    "                record_amp_queue = queue.Queue()\n",
    "                while can_record:\n",
    "                    audio_array = np.append(audio_array, data)\n",
    "                    data, overflowed = voice_input_stream.read(SAMPLING_RATE * REC_DURATION)\n",
    "                    rec_volume_norm = np.linalg.norm(data) * 10\n",
    "                    # determine when to stop recording\n",
    "                    record_amp_queue.put(rec_volume_norm)\n",
    "                    if record_amp_queue.qsize() > 3:\n",
    "                        record_amp = sum(record_amp_queue.queue) / record_amp_queue.qsize()\n",
    "                        # terminate recording if the amplitude back to normal\n",
    "                        if abs(avg_mic_amplitude - record_amp) < RECORD_THRESHOLD:\n",
    "                            can_record = False\n",
    "                            audio_data = {\"array\": audio_array, \"sampling_rate\": SAMPLING_RATE}\n",
    "                            # process audio stream\n",
    "                            # debug_player(audio_data[\"array\"])\n",
    "                            process_audio_stream(audio_data.copy())\n",
    "                            print(\"Recording stopped.\")\n",
    "                            break\n",
    "                        record_amp_queue.get()\n",
    "    finally:\n",
    "        voice_input_stream.stop()\n",
    "        voice_input_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start it UP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPS-Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
