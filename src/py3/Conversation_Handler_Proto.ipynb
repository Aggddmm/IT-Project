{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal imports\n",
    "from LM import LMBackend\n",
    "from TTS import TTSBackend\n",
    "from SpeechRecognition import SpeechRecognitionBackend\n",
    "# external imports\n",
    "from threading import Thread\n",
    "import queue\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] initializing LMBackend\n",
      "    -> Using device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5864db25f6411b955cb31c3c33e04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> LMBackend loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Backend Classes\n",
    "lm_instance = LMBackend()\n",
    "lm_instance.init(\"/Users/lipeihong/Desktop/IT Project/py3/Language_Model/LM/gemma-2-2b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] initializing TTS System\n",
      "    -> Using device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FastSpeech2ConformerHifiGan were not initialized from the model checkpoint at espnet/fastspeech2_conformer_hifigan and are newly initialized: ['mean', 'scale']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> TTS System loaded\n"
     ]
    }
   ],
   "source": [
    "tts_instance = TTSBackend()\n",
    "tts_instance.init(\"espnet/fastspeech2_conformer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] initializing SpeechRecognitionBackend\n",
      "    -> Using device:  mps\n",
      "    -> SpeechRecognitionBackend loaded\n"
     ]
    }
   ],
   "source": [
    "sr_instance = SpeechRecognitionBackend()\n",
    "sr_instance.init(\"/Users/lipeihong/Downloads/whisper-small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "audio_queue = queue.Queue()\n",
    "# Constants\n",
    "RECORD_THRESHOLD = 200\n",
    "SAMPLING_RATE = 16000\n",
    "REC_DURATION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\"\"\" These functions are from Speech-Recognition branch, with process_audio_stream modified to utlise all backends to generate voice response \"\"\"\n",
    "\"\"\" Different ways to coordinate these functions. \"\"\"\n",
    "\n",
    "def audio_callback(indata):\n",
    "    audio_queue.put(indata.copy())  # Put the captured audio\n",
    "\n",
    "def get_mic_amplitude(input_stream, duration):\n",
    "    data, overflowed = input_stream.read(SAMPLING_RATE * duration)\n",
    "    return np.linalg.norm(data) * 10\n",
    "\n",
    "# Code to normalize text(removing non-utf8 character generated by LM)\n",
    "def remove_non_utf8(text):\n",
    "    # Encode the string to bytes, ignoring any non-UTF-8 characters\n",
    "    encoded_text = text.encode(\"utf-8\", \"ignore\")\n",
    "    # Decode it back to a string\n",
    "    decoded_text = encoded_text.decode(\"utf-8\")\n",
    "    return decoded_text\n",
    "\n",
    "def process_audio_stream(audio_input:dict):\n",
    "    \"\"\" from voice input to voice response \"\"\"\n",
    "    # recognize audio\n",
    "    sr_result = sr_instance.recognize(audio_input)\n",
    "    print(\"**** Debug ****: \", sr_result)\n",
    "    # generate response\n",
    "    lm_result = lm_instance.generate_text(sr_result[\"text\"])\n",
    "    \n",
    "    # generate voice response\n",
    "    tts_result = tts_instance.synthesize(remove_non_utf8(lm_result))\n",
    "    \n",
    "    # play audio\n",
    "    sd.play(tts_result[\"array\"], samplerate=tts_result[\"sampling_rate\"])\n",
    "    sd.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_player(audio_data):\n",
    "    \"\"\"Debug function to play the audio from the queue.\"\"\"\n",
    "    print(\"Playing audio...\")\n",
    "    sd.play(audio_data, SAMPLING_RATE)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    voice_input_stream = sd.InputStream(channels=1, samplerate=SAMPLING_RATE)\n",
    "    voice_input_stream.start()\n",
    "    \n",
    "    sound_amp_queue = queue.Queue()\n",
    "    can_record:bool = False\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # detect sound amplitude to determine if we should record\n",
    "            if (sound_amp_queue.qsize() > 15):\n",
    "                sound_amp_queue.get()\n",
    "            \n",
    "            data, overflowed = voice_input_stream.read(SAMPLING_RATE * REC_DURATION)\n",
    "            volume_norm = np.linalg.norm(data) * 10\n",
    "            \n",
    "            if sound_amp_queue.qsize() < 3:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "                continue\n",
    "            \n",
    "            # print(\"amplitude queue: \", sound_amp_queue.queue)\n",
    "            avg_mic_amplitude = sum(sound_amp_queue.queue) / sound_amp_queue.qsize()\n",
    "            # only collect background noise level, not outliers.\n",
    "            if abs(volume_norm - avg_mic_amplitude) > RECORD_THRESHOLD:\n",
    "                can_record = True\n",
    "            else:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "            \n",
    "            if can_record:\n",
    "                print(\"[+] Recording...\")\n",
    "                audio_array = np.empty((0, 1)) \n",
    "                record_amp_queue = queue.Queue()\n",
    "                while can_record:\n",
    "                    audio_array = np.append(audio_array, data)\n",
    "                    data, overflowed = voice_input_stream.read(SAMPLING_RATE * REC_DURATION)\n",
    "                    rec_volume_norm = np.linalg.norm(data) * 10\n",
    "                    # determine when to stop recording\n",
    "                    record_amp_queue.put(rec_volume_norm)\n",
    "                    if record_amp_queue.qsize() > 3:\n",
    "                        record_amp = sum(record_amp_queue.queue) / record_amp_queue.qsize()\n",
    "                        # terminate recording if the amplitude back to normal\n",
    "                        if abs(avg_mic_amplitude - record_amp) < RECORD_THRESHOLD:\n",
    "                            can_record = False\n",
    "                            audio_data = {\"array\": audio_array, \"sampling_rate\": SAMPLING_RATE}\n",
    "                            # process audio stream\n",
    "                            # debug_player(audio_data[\"array\"])\n",
    "                            process_audio_stream(audio_data.copy())\n",
    "                            print(\"Recording stopped.\")\n",
    "                            break\n",
    "                        record_amp_queue.get()\n",
    "    finally:\n",
    "        voice_input_stream.stop()\n",
    "        voice_input_stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Debug ****:  {'text': \" Hello, what's your name?\"}\n",
      "Recording stopped.\n",
      "[+] Recording...\n",
      "**** Debug ****:  {'text': \" It's nice to meet you.\"}\n",
      "Recording stopped.\n",
      "[+] Recording...\n",
      "**** Debug ****:  {'text': ' My name is Ike.'}\n",
      "Recording stopped.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sound_amp_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m     18\u001b[0m     sound_amp_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m---> 20\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mvoice_input_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAMPLING_RATE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mREC_DURATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m volume_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(data) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sound_amp_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sounddevice.py:1464\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1462\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1463\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1464\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1465\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sounddevice.py:1240\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1238\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1239\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1240\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1242\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPS-Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
