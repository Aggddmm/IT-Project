{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal imports\n",
    "from LM import LMBackend\n",
    "from TTS import TTSBackend\n",
    "from SpeechRecognition import SpeechRecognitionBackend\n",
    "from QuestionClassifier import QuestionClassifierBackend\n",
    "from database_mgr import DatabaseQABackend\n",
    "\n",
    "# external imports\n",
    "import queue\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "audio_queue = queue.Queue()\n",
    "# Constants\n",
    "RECORD_THRESHOLD = 200\n",
    "SAMPLING_RATE = 16000\n",
    "REC_DURATION = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start All Backend Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Database\n",
    "db = DatabaseQABackend(\"Database/HistoricalQA_DB.sqlite3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] initializing LMBackend\n",
      "    -> Using device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2bf74a62d04560a126bc9d46ec5c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> LMBackend loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Language Model Backend\n",
    "lm_instance = LMBackend()\n",
    "lm_instance.init(\"/Users/lipeihong/Desktop/IT Project/py3/Language_Model/LM/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] initializing TTS System\n",
      "    -> Using device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FastSpeech2ConformerHifiGan were not initialized from the model checkpoint at espnet/fastspeech2_conformer_hifigan and are newly initialized: ['mean', 'scale']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> TTS System loaded\n"
     ]
    }
   ],
   "source": [
    "# Load TTS Backend\n",
    "tts_instance = TTSBackend()\n",
    "tts_instance.init(\"espnet/fastspeech2_conformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] initializing SpeechRecognitionBackend\n",
      "    -> Using device:  mps\n",
      "    -> SpeechRecognitionBackend loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Speech Recognition Backend\n",
    "sr_instance = SpeechRecognitionBackend()\n",
    "sr_instance.init(\"/Users/lipeihong/Downloads/whisper-small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'historicalQuestion'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Question Classifier Backend\n",
    "qc_instance = QuestionClassifierBackend()\n",
    "qc_instance.init(\"q_classification_model\")\n",
    "\n",
    "qc_instance.classify(\"when did world war 2 end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Functions of Chatting Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\"\"\" These functions are from Speech-Recognition branch, with process_audio_stream modified to utlise all backends to generate voice response \"\"\"\n",
    "\"\"\" Different ways to coordinate these functions. \"\"\"\n",
    "\n",
    "def audio_callback(indata):\n",
    "    audio_queue.put(indata.copy())  # Put the captured audio\n",
    "\n",
    "def get_mic_amplitude(input_stream, duration):\n",
    "    data, overflowed = input_stream.read(SAMPLING_RATE * duration)\n",
    "    return np.linalg.norm(data) * 10\n",
    "\n",
    "def process_audio_stream(audio_input:dict) -> None:\n",
    "    \"\"\" from voice input to voice response \"\"\"\n",
    "    # recognize audio\n",
    "    sr_result = sr_instance.recognize(audio_input)[\"text\"]\n",
    "    print(\"**** Debug ****: \", sr_result)\n",
    "    # classify question\n",
    "    question_type = qc_instance.classify(sr_result)\n",
    "    print(\"**** Debug ****: \", question_type)\n",
    "    \n",
    "    if question_type == \"historicalQuestion\":\n",
    "        # TODO: add historical question answering pipeline.\n",
    "        # Logic: 1. Serch clue in database\n",
    "        #        2. Generate prompt\n",
    "        #        3. update sr_result, which is the prompt.\n",
    "        db_result = db.get_answer(sr_result)\n",
    "        print(\"**** Debug ****: \")\n",
    "        for row in db_result:\n",
    "            print(\"    \" + row)\n",
    "        print(\"**** Debug ****\")\n",
    "        exit(0)\n",
    "        pass\n",
    "    \n",
    "    # generate response\n",
    "    lm_result = lm_instance.generate_text(sr_result)\n",
    "    \n",
    "    # generate voice response\n",
    "    tts_result = tts_instance.synthesize(lm_result)\n",
    "    \n",
    "    # play audio\n",
    "    sd.play(tts_result[\"array\"], samplerate=tts_result[\"sampling_rate\"])\n",
    "    sd.wait()\n",
    "\n",
    "def debug_player(audio_data):\n",
    "    \"\"\"Debug function to play the audio from the queue.\"\"\"\n",
    "    print(\"Playing audio...\")\n",
    "    sd.play(audio_data, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedicated Debug Block For main function\n",
    "cuz bugs really easy to 'be produced' in this block...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" This function controls when to record and when to stop recording \"\"\"\n",
    "    voice_input_stream = sd.InputStream(channels=1, samplerate=SAMPLING_RATE)\n",
    "    voice_input_stream.start()\n",
    "    \n",
    "    sound_amp_queue = queue.Queue()\n",
    "    can_record:bool = False\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # detect sound amplitude to determine if we should record\n",
    "            if (sound_amp_queue.qsize() > 15):\n",
    "                sound_amp_queue.get()\n",
    "            \n",
    "            data, overflowed = voice_input_stream.read(SAMPLING_RATE * REC_DURATION)\n",
    "            volume_norm = np.linalg.norm(data) * 10\n",
    "            \n",
    "            if sound_amp_queue.qsize() < 3:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "                continue\n",
    "            \n",
    "            # print(\"amplitude queue: \", sound_amp_queue.queue)\n",
    "            avg_mic_amplitude = sum(sound_amp_queue.queue) / sound_amp_queue.qsize()\n",
    "            # only collect background noise level, not outliers.\n",
    "            if abs(volume_norm - avg_mic_amplitude) > RECORD_THRESHOLD:\n",
    "                can_record = True\n",
    "            else:\n",
    "                sound_amp_queue.put(volume_norm)\n",
    "            \n",
    "            if can_record:\n",
    "                print(\"[+] Recording...\")\n",
    "                audio_array = np.empty((0, 1)) \n",
    "                record_amp_queue = queue.Queue()\n",
    "                while can_record:\n",
    "                    audio_array = np.append(audio_array, data)\n",
    "                    data, overflowed = voice_input_stream.read(SAMPLING_RATE * REC_DURATION)\n",
    "                    rec_volume_norm = np.linalg.norm(data) * 10\n",
    "                    # determine when to stop recording\n",
    "                    record_amp_queue.put(rec_volume_norm)\n",
    "                    if record_amp_queue.qsize() > 3:\n",
    "                        record_amp = sum(record_amp_queue.queue) / record_amp_queue.qsize()\n",
    "                        # terminate recording if the amplitude back to normal\n",
    "                        if abs(avg_mic_amplitude - record_amp) < RECORD_THRESHOLD:\n",
    "                            can_record = False\n",
    "                            audio_data = {\"array\": audio_array, \"sampling_rate\": SAMPLING_RATE}\n",
    "                            # process audio stream\n",
    "                            # debug_player(audio_data[\"array\"])\n",
    "                            process_audio_stream(audio_data.copy())\n",
    "                            print(\"Recording stopped.\")\n",
    "                            break\n",
    "                        record_amp_queue.get()\n",
    "    finally:\n",
    "        voice_input_stream.stop()\n",
    "        voice_input_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start it UP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Recording...\n",
      "**** Debug ****:   When did World War 2 end?\n",
      "**** Debug ****:  historicalQuestion\n",
      "**** Debug ****:  ['The killing heightened tensions between the U.S. and Iran, leading to military confrontations, regional instability, and global concerns about escalation.', 'The killing heightened tensions between the U.S. and Iran, leading to military confrontations, regional instability, and global concerns about escalation.', 'Through military conquest, administrative reforms, and the establishment of a unified code of laws.', 'Through military conquest, administrative reforms, and the establishment of a unified code of laws.', 'She expanded trade networks, commissioned monumental architecture, and maintained peace during her reign.']\n",
      "    -> Generated response:  1945! ðŸ’¥  \n",
      "Recording stopped.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sound_amp_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m     13\u001b[0m     sound_amp_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m---> 15\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mvoice_input_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAMPLING_RATE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mREC_DURATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m volume_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(data) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sound_amp_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sounddevice.py:1464\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1462\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1463\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1464\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1465\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MPS-Torch/lib/python3.12/site-packages/sounddevice.py:1240\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1238\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1239\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1240\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1242\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPS-Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
